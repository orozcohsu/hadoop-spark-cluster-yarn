1. Try spark cluster
1-1. docker-compose up -d
1-2. docker exec -it datanode-worker bash
1-3. RUN IT AS client mode:  ./spark-submit --class org.apache.spark.examples.SparkPi --master spark://namenode-master:7077 --executor-memory 1G --total-executor-cores 1 ../examples/jars/spark-examples_2.11-2.4.1.jar 1000
1-4. RUN IT AS cluster mode: ./spark-submit --class org.apache.spark.examples.SparkPi --master spark://namenode-master:7077 --deploy-mode cluster --executor-memory 1G --total-executor-cores 1 ../examples/jars/spark-examples_2.11-2.4.1.jar 1000
1-5. CHECK 8080 and SEE APPLICATION STATUS

2. Try spark yarn
1-1. RUN IT AS yarn(client): ./spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --executor-memory 1G --executor-cores 1 ../examples/jars/spark-examples_2.11-2.4.1.jar 1000
1-2. RUN IT AS yarn(client):


all reference:
https://taiwansparkusergroup.gitbooks.io/spark-programming-guide-zh-tw/content/deploying/submitting-applications.html
